{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwa5tM8b4agjhXFeIoQhjE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surya2004-janardhan/colab/blob/main/taskactionpipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task: Extract and Categorize Tasks from Unannotated Text**\n"
      ],
      "metadata": {
        "id": "-PFoiS7uT8Xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing neccessary modules and packages"
      ],
      "metadata": {
        "id": "9z1x6-RlUFbT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrNU_fsFTLfL"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This section cleans the input text by converting it to lowercase, tokenizing it into words, and removing punctuation and non-alphanumeric characters. The result is a list of clean tokens ready for further processing."
      ],
      "metadata": {
        "id": "mF0hBBizUNqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "FkdKY7WOT6dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function identifies actionable sentences as tasks using heuristic rules. It assigns scores based on keywords like deadlines, assignments, and entity mentions. Sentences scoring above a threshold are considered tasks"
      ],
      "metadata": {
        "id": "oBkLpt1aURRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Task Identification\n",
        "def identify_tasks(sentences):\n",
        "    tasks = []\n",
        "    deadline_keywords = [\"by\", \"before\", \"after\", \"tomorrow\", \"today\", \"this evening\"]\n",
        "    assignment_keywords = [\"has to\", \"needs to\", \"is responsible for\", \"should\", \"must\", \"need to\"]\n",
        "\n",
        "    for sentence in sentences:\n",
        "        score = 0\n",
        "        if any(sentence.startswith(verb) for verb in [\"buy\", \"clean\", \"submit\", \"send\"]):\n",
        "            score += 1\n",
        "        if any(word in sentence for word in assignment_keywords):\n",
        "            score += 1\n",
        "        if any(word in sentence for word in deadline_keywords):\n",
        "            score += 1\n",
        "        if \"rahul\" in sentence.lower():\n",
        "            score += 1\n",
        "        if score >= 2:\n",
        "            tasks.append(sentence.strip())\n",
        "    return tasks"
      ],
      "metadata": {
        "id": "qJ4KhvGGT6Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section generates word embeddings for tasks using Word2Vec and clusters them using K-Means. Tasks are grouped into meaningful categories based on their semantic similarity, with numerical clusters renamed to descriptive labels like \"Category A.\""
      ],
      "metadata": {
        "id": "raY6_ATJUX8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Categorization with Word Embeddings\n",
        "def cluster_tasks_with_word_embeddings(tasks):\n",
        "    if len(tasks) <= 1:\n",
        "        return {\"Uncategorized\": tasks}\n",
        "\n",
        "    tokenized_tasks = [task.split() for task in tasks]\n",
        "    model = Word2Vec(sentences=tokenized_tasks, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    embeddings = []\n",
        "    for task in tokenized_tasks:\n",
        "        vec = np.mean([model.wv[word] for word in task if word in model.wv], axis=0)\n",
        "        embeddings.append(vec)\n",
        "\n",
        "    n_clusters = min(len(tasks), 3)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    categories = {}\n",
        "    for i, task in enumerate(tasks):\n",
        "        cluster_id = clusters[i]\n",
        "        if cluster_id not in categories:\n",
        "            categories[cluster_id] = []\n",
        "        categories[cluster_id].append(task)\n",
        "\n",
        "    renamed_categories = {}\n",
        "    category_names = [\"Category A\", \"Category B\", \"Category C\"][:len(categories)]\n",
        "    for i, (key, value) in enumerate(categories.items()):\n",
        "        renamed_categories[category_names[i]] = value\n",
        "    return renamed_categories"
      ],
      "metadata": {
        "id": "8Km--ug8T6Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function uses Latent Dirichlet Allocation (LDA) to dynamically discover topics in the tasks. Tasks are assigned to the most probable topic, and the numerical topics are renamed to descriptive labels like \"Topic A.\""
      ],
      "metadata": {
        "id": "_O4vapaDUczY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Categorization with LDA\n",
        "def categorize_tasks_with_lda(tasks):\n",
        "    if not tasks:\n",
        "        return {\"Uncategorized\": []}\n",
        "\n",
        "    if len(tasks) <= 1:\n",
        "        return {\"Uncategorized\": tasks}\n",
        "\n",
        "    vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
        "    X = vectorizer.fit_transform(tasks)\n",
        "\n",
        "    n_topics = min(len(tasks), 3)\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "    lda.fit(X)\n",
        "\n",
        "    topic_probabilities = lda.transform(X)\n",
        "    categories = {}\n",
        "    for i, task in enumerate(tasks):\n",
        "        topic = topic_probabilities[i].argmax()\n",
        "        if topic not in categories:\n",
        "            categories[topic] = []\n",
        "        categories[topic].append(task)\n",
        "\n",
        "    renamed_categories = {}\n",
        "    category_names = [\"Topic A\", \"Topic B\", \"Topic C\"][:len(categories)]\n",
        "    for i, (key, value) in enumerate(categories.items()):\n",
        "        renamed_categories[category_names[i]] = value\n",
        "    return renamed_categories"
      ],
      "metadata": {
        "id": "ayJV52jtT6Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function combines all previous steps to generate the final output. It extracts tasks, categorizes them using both word embeddings (K-Means) and LDA, and formats the results into a structured dictionary."
      ],
      "metadata": {
        "id": "zv39BzsFUiP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Output Generation\n",
        "def generate_output(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    tasks = identify_tasks(sentences)\n",
        "\n",
        "    if not tasks:\n",
        "        return {\n",
        "            \"Tasks\": [],\n",
        "            \"Categories_Word_Embeddings\": {},\n",
        "            \"Categories_LDA\": {}\n",
        "        }\n",
        "\n",
        "    categories_word_embeddings = cluster_tasks_with_word_embeddings(tasks)\n",
        "    categories_lda = categorize_tasks_with_lda(tasks)\n",
        "\n",
        "    output = {\n",
        "        \"Tasks\": [{\"Task\": task} for task in tasks],\n",
        "        \"Categories_Word_Embeddings\": categories_word_embeddings,\n",
        "        \"Categories_LDA\": categories_lda\n",
        "    }\n",
        "    return output"
      ],
      "metadata": {
        "id": "CNGMjk8mT6O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section demonstrates the pipeline's functionality with an example input text. It prints the identified tasks and their categories derived from word embeddings and LDA in a readable format."
      ],
      "metadata": {
        "id": "JxOgDcKVUpre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"\"\"\n",
        "    Rahul bought groceries yesterday. Today, he plans to organize his study room.\n",
        "    He needs to send an email to his manager by noon. Also, Rahul has to pick up his dry cleaning by 6 pm.\n",
        "    Tomorrow, Rahul must prepare a presentation for the team meeting. In the evening, he will meet friends at the café.\"\"\"\n",
        "    result = generate_output(text)\n",
        "    print(\"Output:\")\n",
        "    print(\"------\")\n",
        "    print(\"Tasks:\")\n",
        "    for task in result[\"Tasks\"]:\n",
        "        print(f\"- {task['Task']}\")\n",
        "    print(\"\\nCategories (Word Embeddings):\")\n",
        "    for category, tasks in result[\"Categories_Word_Embeddings\"].items():\n",
        "        print(f\"- {category}: {', '.join(tasks)}\")\n",
        "    print(\"\\nCategories (LDA):\")\n",
        "    for category, tasks in result[\"Categories_LDA\"].items():\n",
        "        print(f\"- {category}: {', '.join(tasks)}\")"
      ],
      "metadata": {
        "id": "Y_KqB2gJT6Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Key Sections\n",
        "Preprocessing : Cleans and tokenizes the input text.\n",
        "\n",
        "\n",
        "Task Identification : Extracts actionable sentences as tasks using heuristic rules.\n",
        "\n",
        "\n",
        "Word Embeddings Clustering : Groups tasks into categories based on semantic similarity using K-Means.\n",
        "\n",
        "LDA Topic Modeling : Dynamically discovers topics in tasks and assigns them to categories.\n",
        "\n",
        "\n",
        "Output Generation : Combines all results into a structured dictionary and formats the output for readability.\n",
        "\n",
        "\n",
        "Example Usage : Demonstrates the pipeline with a sample input and prints the results.\n",
        "This modular approach ensures clarity, flexibility, and robustness in extracting and categorizing tasks from unstructured text.**\n"
      ],
      "metadata": {
        "id": "q6tVj22eUyxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kOO4pWzUUtn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall code :**"
      ],
      "metadata": {
        "id": "02GTS5WjVL_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    return tokens\n",
        "\n",
        "# Step 2: Task Identification\n",
        "def identify_tasks(sentences):\n",
        "    tasks = []\n",
        "    deadline_keywords = [\"by\", \"before\", \"after\", \"tomorrow\", \"today\", \"this evening\"]\n",
        "    assignment_keywords = [\"has to\", \"needs to\", \"is responsible for\", \"should\", \"must\", \"need to\"]\n",
        "\n",
        "    for sentence in sentences:\n",
        "        score = 0\n",
        "        if any(sentence.startswith(verb) for verb in [\"buy\", \"clean\", \"submit\", \"send\"]):\n",
        "            score += 1\n",
        "        if any(word in sentence for word in assignment_keywords):\n",
        "            score += 1\n",
        "        if any(word in sentence for word in deadline_keywords):\n",
        "            score += 1\n",
        "        if \"rahul\" in sentence.lower():\n",
        "            score += 1\n",
        "        if score >= 2:\n",
        "            tasks.append(sentence.strip())  # Remove extra whitespace\n",
        "    return tasks\n",
        "\n",
        "# Step 3: Categorization with Word Embeddings\n",
        "def cluster_tasks_with_word_embeddings(tasks):\n",
        "    if len(tasks) <= 1:  # Skip clustering if there's only one task\n",
        "        return {\"Uncategorized\": tasks}\n",
        "\n",
        "    tokenized_tasks = [task.split() for task in tasks]\n",
        "    model = Word2Vec(sentences=tokenized_tasks, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    embeddings = []\n",
        "    for task in tokenized_tasks:\n",
        "        vec = np.mean([model.wv[word] for word in task if word in model.wv], axis=0)\n",
        "        embeddings.append(vec)\n",
        "\n",
        "    # Dynamically adjust n_clusters\n",
        "    n_clusters = min(len(tasks), 3)  # Limit to 3 clusters or the number of tasks\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    categories = {}\n",
        "    for i, task in enumerate(tasks):\n",
        "        cluster_id = clusters[i]\n",
        "        if cluster_id not in categories:\n",
        "            categories[cluster_id] = []\n",
        "        categories[cluster_id].append(task)\n",
        "\n",
        "    # Rename numerical keys to descriptive category names\n",
        "    renamed_categories = {}\n",
        "    category_names = [\"Category A\", \"Category B\", \"Category C\"][:len(categories)]\n",
        "    for i, (key, value) in enumerate(categories.items()):\n",
        "        renamed_categories[category_names[i]] = value\n",
        "    return renamed_categories\n",
        "\n",
        "# Step 4: Categorization with LDA\n",
        "def categorize_tasks_with_lda(tasks):\n",
        "    if not tasks:\n",
        "        return {\"Uncategorized\": []}\n",
        "\n",
        "    if len(tasks) <= 1:  # Skip LDA if there's only one task\n",
        "        return {\"Uncategorized\": tasks}\n",
        "\n",
        "    vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
        "    X = vectorizer.fit_transform(tasks)\n",
        "\n",
        "    # Dynamically adjust n_components\n",
        "    n_topics = min(len(tasks), 3)  # Limit to 3 topics or the number of tasks\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "    lda.fit(X)\n",
        "\n",
        "    topic_probabilities = lda.transform(X)\n",
        "    categories = {}\n",
        "    for i, task in enumerate(tasks):\n",
        "        topic = topic_probabilities[i].argmax()\n",
        "        if topic not in categories:\n",
        "            categories[topic] = []\n",
        "        categories[topic].append(task)\n",
        "\n",
        "    # Rename numerical keys to descriptive category names\n",
        "    renamed_categories = {}\n",
        "    category_names = [\"Topic A\", \"Topic B\", \"Topic C\"][:len(categories)]\n",
        "    for i, (key, value) in enumerate(categories.items()):\n",
        "        renamed_categories[category_names[i]] = value\n",
        "    return renamed_categories\n",
        "\n",
        "# Step 5: Output Generation\n",
        "def generate_output(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    tasks = identify_tasks(sentences)\n",
        "\n",
        "    if not tasks:  # Handle case where no tasks are identified\n",
        "        return {\n",
        "            \"Tasks\": [],\n",
        "            \"Categories_Word_Embeddings\": {},\n",
        "            \"Categories_LDA\": {}\n",
        "        }\n",
        "\n",
        "    categories_word_embeddings = cluster_tasks_with_word_embeddings(tasks)\n",
        "    categories_lda = categorize_tasks_with_lda(tasks)\n",
        "\n",
        "    output = {\n",
        "        \"Tasks\": [{\"Task\": task} for task in tasks],\n",
        "        \"Categories_Word_Embeddings\": categories_word_embeddings,\n",
        "        \"Categories_LDA\": categories_lda\n",
        "    }\n",
        "    return output\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"\"\"Rahul finished his homework last night. In the morning, he needs to drop off the package at the post office.\n",
        "Later, Rahul has to call the plumber to fix the leaking tap. By 2 pm, he must finish drafting the project proposal.\n",
        "In the evening, Rahul plans to cook dinner for his family. Also, he should water the plants before leaving home..\"\"\"\n",
        "    result = generate_output(text)\n",
        "    print(\"Output:\")\n",
        "    print(\"------\")\n",
        "    print(\"Tasks:\")\n",
        "    for task in result[\"Tasks\"]:\n",
        "        print(f\"- {task['Task']}\")\n",
        "    print(\"\\nCategories (Word Embeddings):\")\n",
        "    for category, tasks in result[\"Categories_Word_Embeddings\"].items():\n",
        "        print(f\"- {category}: {', '.join(tasks)}\")\n",
        "    print(\"\\nCategories (LDA):\")\n",
        "    for category, tasks in result[\"Categories_LDA\"].items():\n",
        "        print(f\"- {category}: {', '.join(tasks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqlzDdcHT6ED",
        "outputId": "c368885b-2e36-4682-9a66-5b4ba1ae48e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "------\n",
            "Tasks:\n",
            "- Later, Rahul has to call the plumber to fix the leaking tap.\n",
            "- Also, he should water the plants before leaving home..\n",
            "\n",
            "Categories (Word Embeddings):\n",
            "- Category A: Later, Rahul has to call the plumber to fix the leaking tap.\n",
            "- Category B: Also, he should water the plants before leaving home..\n",
            "\n",
            "Categories (LDA):\n",
            "- Topic A: Later, Rahul has to call the plumber to fix the leaking tap.\n",
            "- Topic B: Also, he should water the plants before leaving home..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output 1 :\n",
        "\n",
        "Tasks:\n",
        "- He needs to send an email to his manager by noon.\n",
        "- Also, Rahul has to pick up his dry cleaning by 6 pm.\n",
        "- Tomorrow, Rahul must prepare a presentation for the team meeting.\n",
        "\n",
        "Categories (Word Embeddings):\n",
        "- Category A: He needs to send an email to his manager by noon.\n",
        "- Category B: Also, Rahul has to pick up his dry cleaning by 6 pm.\n",
        "- Category C: Tomorrow, Rahul must prepare a presentation for the team meeting.\n",
        "\n",
        "Categories (LDA):\n",
        "- Topic A: He needs to send an email to his manager by noon., Also, Rahul has to pick up his dry cleaning by 6 pm.\n",
        "- Topic B: Tomorrow, Rahul must prepare a presentation for the team meeting.\n",
        "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
        "[nltk_data]   Package punkt is already up-to-date!\n",
        "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
        "[nltk_data]   Package punkt_tab is already up-to-date!"
      ],
      "metadata": {
        "id": "Xrmin0NHVTZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output 2:\n",
        "\n",
        "\n",
        "Later, Rahul has to call the plumber to fix the leaking tap. By 2 pm, he must finish drafting the project proposal.\n",
        "In the evening, Rahul plans to cook dinner for his family. Also, he should water the plants before leaving home..\"\"\"\n",
        "\n",
        "Output:\n",
        "------\n",
        "Tasks:\n",
        "- Later, Rahul has to call the plumber to fix the leaking tap.\n",
        "- Also, he should water the plants before leaving home..\n",
        "\n",
        "Categories (Word Embeddings):\n",
        "- Category A: Later, Rahul has to call the plumber to fix the leaking tap.\n",
        "- Category B: Also, he should water the plants before leaving home..\n",
        "\n",
        "Categories (LDA):\n",
        "- Topic A: Later, Rahul has to call the plumber to fix the leaking tap.\n",
        "- Topic B: Also, he should water the plants before leaving home..\n",
        "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
        "[nltk_data]   Package punkt is already up-to-date!\n",
        "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
        "[nltk_data]   Package punkt_tab is already up-to-date!"
      ],
      "metadata": {
        "id": "UVsOpCVyWh2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challeneges:**\n",
        "\n",
        "1. **Task Identification**: Developing heuristics to accurately detect tasks amidst varied sentence structures and ambiguous language.\n",
        "2. **Small Dataset Issues**: Clustering algorithms struggled with limited data, requiring dynamic adjustments to parameters like `n_clusters` and `n_topics`.\n",
        "3. **Sparse Embeddings**: Limited vocabulary in some tasks led to sparse word embeddings, reducing clustering effectiveness.\n",
        "4. **Interpretability**: Ensuring clusters and topics from LDA and K-Means were meaningful and user-friendly required additional renaming logic.\n",
        "5. **Edge Cases & Dependencies**: Handling edge cases (e.g., no tasks or one task) and managing external library dependencies added complexity to the pipeline."
      ],
      "metadata": {
        "id": "Ds8FwOOuW8OR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hMcOKJonXQUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jsZKizDVY4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}