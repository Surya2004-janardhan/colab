{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3EOlZcZidqDANqTD26sQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surya2004-janardhan/colab/blob/main/sentimentanalysis_imdb_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Necessary Modules\n",
        "Import essential libraries for data handling, preprocessing, feature extraction, model training, and evaluation. These include Pandas, NLTK, Scikit-learn, and others. This step ensures all required tools are available for the pipeline."
      ],
      "metadata": {
        "id": "BNZ8fFls8gmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e07oe_QB8hyp",
        "outputId": "3ba59bcf-3b06-4c89-e368-71ec86099ac1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load Dataset with Robust Settings\n",
        "Load the dataset using robust settings to handle encoding, quoting issues, and malformed rows. If parsing fails, clean the dataset line by line and reload it. This ensures the dataset is error-free and ready for processing."
      ],
      "metadata": {
        "id": "PvlEfLjz8sUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, encoding='utf-8', engine='python', on_bad_lines=\"skip\")\n",
        "    except pd.errors.ParserError:\n",
        "        print(\"Parser Error detected. Cleaning the dataset...\")\n",
        "        cleaned_data = []\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    cleaned_line = line.replace('\\x00', '').strip()\n",
        "                    if cleaned_line:\n",
        "                        cleaned_data.append(cleaned_line)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in row {i}: {e}\")\n",
        "        cleaned_file_path = 'cleaned_' + file_path.split('/')[-1]\n",
        "        with open(cleaned_file_path, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(cleaned_data))\n",
        "        df = pd.read_csv(cleaned_file_path)\n",
        "    return df"
      ],
      "metadata": {
        "id": "fDQ2Sirc8upZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tep 3: Data Cleaning and Preprocessing\n",
        "Preprocess the text data by converting it to lowercase, removing non-alphabetic characters, tokenizing, and eliminating stop words. This step ensures the data is clean and consistent for further analysis."
      ],
      "metadata": {
        "id": "7g9jPxN38wf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens) if tokens else ''"
      ],
      "metadata": {
        "id": "baLKLHM_81Uw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Feature Extraction\n",
        "Convert the cleaned text into numerical features using TF-IDF with bigrams. This step captures both unigrams and bigrams, reducing noise and improving context representation."
      ],
      "metadata": {
        "id": "5iG1aBZ883OL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Sdrc2C-9WIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(corpus):\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "    max_features=10000, # Increase feature limit\n",
        "    ngram_range=(1, 2), # Use unigrams and bigrams\n",
        "    min_df=2, # Ignore terms appearing in fewer than 2 documents\n",
        "    max_df=0.9 # Ignore terms appearing in more than 90% of documents\n",
        "    )\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    return X, vectorizer"
      ],
      "metadata": {
        "id": "8hOdQdFJ9VMX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Model Training with Hyperparameter Tuning (Continued)\n",
        "Train a Logistic Regression model with hyperparameter tuning using Grid Search. This step optimizes the model's performance by finding the best parameters for regularization strength and class weighting."
      ],
      "metadata": {
        "id": "osrlncUj9bGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'C': [0.01, 0.1, 1, 10],\n",
        "        'solver': ['liblinear'],\n",
        "        'class_weight': [None, 'balanced']\n",
        "    }\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best F1-Score (CV): {grid_search.best_score_:.2f}\")\n",
        "    return grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "goQCycUC9Wkt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Step 6: Evaluation\n",
        "Evaluate the trained model on the test set using metrics like accuracy, precision, recall, and F1-score. A detailed classification report provides insights into the model's performance across classes."
      ],
      "metadata": {
        "id": "B_kCZs4R9vhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ujeWcOTM9ynY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Main Execution\n",
        "Combine all steps into a single pipeline. Load the dataset, preprocess the data, extract features, split the data into training and testing sets, train the model, and evaluate its performance."
      ],
      "metadata": {
        "id": "Pt-eBwfp93dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    file_path = '/content/IMDB Dataset.csv'\n",
        "    df = load_dataset(file_path)\n",
        "    print(f\"Dataset Size: {len(df)}\")\n",
        "    print(\"First 5 Rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "    df = df.dropna(subset=['review', 'sentiment'])\n",
        "    df = df[df['sentiment'].isin(['positive', 'negative'])]\n",
        "    df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "    df['clean_review'] = df['review'].apply(preprocess_text)\n",
        "    df = df[df['clean_review'] != '']\n",
        "\n",
        "    X, vectorizer = extract_features(df['clean_review'])\n",
        "    y = df['sentiment']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    model = train_model(X_train, y_train)\n",
        "    evaluate_model(model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp0a9sFR94aW",
        "outputId": "9fc8e0cc-0936-41ae-f479-3b6968148fdc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size: 7886\n",
            "First 5 Rows:\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "Best Parameters: {'C': 10, 'class_weight': None, 'solver': 'liblinear'}\n",
            "Best F1-Score (CV): 0.88\n",
            "Accuracy: 0.88\n",
            "Precision: 0.87\n",
            "Recall: 0.89\n",
            "F1-Score: 0.88\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.87      0.88       787\n",
            "           1       0.87      0.89      0.88       791\n",
            "\n",
            "    accuracy                           0.88      1578\n",
            "   macro avg       0.88      0.88      0.88      1578\n",
            "weighted avg       0.88      0.88      0.88      1578\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall Summary\n",
        "This pipeline efficiently handles text classification tasks by loading a robust dataset, preprocessing text data to remove noise, extracting meaningful TF-IDF features, tuning hyperparameters for optimal performance, and evaluating the model comprehensively. The use of bigrams in TF-IDF and Grid Search ensures high-quality feature representation and model optimization. The pipeline is modular, scalable, and adaptable to various text classification problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "5cted41X-UB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Challenges Encountered\n",
        "\n",
        "Malformed Dataset : Handling quoting issues and encoding errors required robust loading mechanisms.\n",
        "\n",
        "\n",
        "Class Imbalance : Addressed using class_weight='balanced' to ensure fair treatment of both classes.\n",
        "\n",
        "\n",
        "Preprocessing Complexity : Removing stop words and tokenizing while preserving context was challenging but resolved with NLTK tools.\n",
        "\n",
        "\n",
        "Computational Cost : Large datasets increased processing time, mitigated by limiting TF-IDF features and using efficient algorithms.\n",
        "\n",
        "\n",
        "Hyperparameter Tuning : Finding optimal parameters for the model required careful experimentation and validation."
      ],
      "metadata": {
        "id": "S24ZtFG1-bGl"
      }
    }
  ]
}